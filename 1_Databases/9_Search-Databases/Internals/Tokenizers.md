# Tokenizers
- [Tokenizers](https://lucene.apache.org/core/7_3_1/core/org/apache/lucene/analysis/Tokenizer.html) are used to generate the tokens from a text string/document.
- It breaks down the text string into tokens where it finds whitespace or other punctuation symbols.